action.BigDataTools.CreateClusterConnectionsAction.text=Nueva conexión
action.BigDataTools.Deploy.Configure.text=Crear configuración de Spark Submit...
add.new.arbitrary.cluster.submit.connection.label=Agregar cluster Spark personalizado...
add.new.ssh.connection.label=Agregar conexión SSH...
app.by.me.value=Por mí
arbitrary.cluster.wizard.create.connection.title=Cluster Spark personalizado
arbitrary.cluster.wizard.default.radio.button=Usar configuración predeterminada
arbitrary.cluster.wizard.select.sftp.step.desc=Configurar conexión SFTP al nodo driver
arbitrary.cluster.wizard.select.sftp.step.title=SFTP
arbitrary.cluster.wizard.select.spark.step.desc=Configurar conexión al Spark History Server
arbitrary.cluster.wizard.select.spark.step.title=Spark
arbitrary.cluster.wizard.select.ssh.step.desc=Especificar configuración SSH para usar Spark-Submit
arbitrary.cluster.wizard.select.ssh.step.title=SSH
arbitrary.cluster.wizard.sftp.custom.radio.button=Configurar conexión personalizada
arbitrary.cluster.wizard.sftp.none.radio.button=No necesito conexión SFTP al nodo driver
arbitrary.cluster.wizard.spark.custom.radio.button=Configurar conexión personalizada
arbitrary.cluster.wizard.spark.none.radio.button=No necesito conexión al Spark History Server
arbitrary.cluster.wizard.spark.step.default.uri.comment.text=(localhost en el host del túnel)
artifact.tooltip=Ruta al ejecutable para ejecutar en el cluster
check.connection.availability=Verificar disponibilidad de conexión
check.ssh.connection.ssh.is.not.defined=Configuración SSH no especificada
cluster.manager.kubernetes=Kubernetes
cluster.manager.local=Local
cluster.manager.mesos=Apache Mesos
cluster.manager.nomad=Nomad
cluster.manager.standalone=Standalone
cluster.manager.tooltip=Gestor de cluster configurado en el servidor.
cluster.manager.yarn=Hadoop YARN
cluster.status.loading=Cargando...
cluster.status.no.target=<Sin destino>
cluster.status.not.selected=<No seleccionado>
configuration.description=Configuración de Spark Submit
configuration.name=Spark Submit
configuration.name.cluster=Cluster
configuration.name.local=Local (Obsoleto)
configuration.name.python=PySpark
configuration.name.ssh=SSH (Obsoleto)
configuration.options.add=Personalización adicional
configuration.options.add.title=Agregar opciones de submit
dialog.archives.title=Seleccionar archivos
dialog.artifactPath.title=Seleccionar aplicación
dialog.driverClassPath.title=Seleccionar classpath del driver
dialog.driverLibraryPath.title=Seleccionar ruta de biblioteca del driver
dialog.files.title=Seleccionar archivos
dialog.input.spark.command.description=Pegar comando spark-submit aquí para llenar el formulario de Spark Submit
dialog.input.spark.command.label=Comando Spark
dialog.input.spark.command.title=Entrada Spark
dialog.jars.title=Seleccionar archivos Jar
dialog.keytabFile.title=Seleccionar archivo keytab
dialog.message.adding.other.configurations.not.allowed.here=No se permite agregar otras configuraciones aquí.
dialog.message.failed.to.create.ssh.process=Error al crear proceso SSH
dialog.message.not.found=No se encontró {0}
dialog.message.spark.home.should.be.set.to.correct.folder=$SPARK_HOME debe establecerse al directorio correcto.
dialog.message.specify.application=Especificar aplicación
dialog.message.ssh.configuration.changed=Ha seleccionado otra configuración SSH para el cluster {0}.
dialog.propertiesFile.title=Seleccionar archivo de propiedades
dialog.pyfiles.title=Seleccionar archivos Python
dialog.select.artifact.button.open.artifact.settings=Configuración de artifacts
dialog.select.artifact.empty=No hay artifacts en el proyecto actual
dialog.select.artifact.link.open.artifact.settings=Configurar artifacts
dialog.select.artifact.title=Seleccionar artifact de dependencia
dialog.select.class.empty=No se encontraron clases en la aplicación seleccionada
dialog.select.class.title=Nombre de clase
dialog.sparkHomePath.title=Seleccionar directorio principal de Spark
dialog.targetDirectory.title=Seleccionar directorio destino
dialog.title.select.gradle.artifact.with.task=Seleccionar artifact y tarea de Gradle
dialog.title.ssh.configuration.changed=Configuración SSH modificada
dialog.workDir.title=Seleccionar directorio de trabajo
edit.ssh.configuration=Editar configuración SSH
error.ssh=Especificar configuración SSH
error.ssh.config=Se debe especificar configuración SSH
error.target.config=Seleccionar destino remoto
error.target.config.unresolved=Seleccionar destino remoto válido
exportable.SparkSubmitSettings.presentable.name=Configuración de Big Data Tools Spark Submit
fun.search.process.text=Proceso {0}
group.ServiceView.AddSparkService.text=Cluster Spark
inlay.attach.debugger.ssh=Adjuntar debugger a través de túnel SSH
label.implicit.cluster.depend.sftp=SFTP\:
label.implicit.cluster.depend.spark.connection=Spark History\:
label.select.ssh.configuration.for.cluster=Seleccionar configuración SSH para cluster {0}
load.command.string=Cargar comando spark-submit
notification.group.sftpsparkfileupload=Carga de archivos SFTP para Spark
open.cluster.info.description=Clic para abrir documentación
progress.text.upload.to.host=Subiendo {0} al host...
pyspark=PySpark
pyspark.documentation.dataframe.schema=Columnas\: {0}
receive.artifact.task=Recibiendo artifact...
remote.target.arbitrary.cluster.remark=Cluster personalizado
remote.target.ssh.remark=SSH
replace.with.allowed.value=Reemplazar con valor permitido
row.final.command=Comando de submit resultante
row.final.command.copy=Copiar comando spark-submit
row.final.command.hint=Puede copiar o cargar un comando spark-submit que rellenará los campos correspondientes
services.error.create.spark.connection.canceled.fix.label=Intentar recargar
services.error.create.spark.connection.canceled.message=Cancelado por el usuario
services.error.ssh.config.is.not.found=Configuración SSH no encontrada
services.error.top.cannot.create.spark.connection=No se puede crear conexión Spark
services.spark.connection.is.not.created=No se pudo crear conexión Spark
settings.additional.title=Opciones avanzadas de submit
settings.additional.verbose=Imprimir salida de debug adicional
settings.application=Aplicación\:
settings.application.arguments=Argumentos de ejecución\:
settings.application.class.hint=Clase principal de su aplicación para --class (para aplicaciones Java/Scala).
settings.application.class.name=Clase\:
settings.application.class.name.error.msg=Especifique la aplicación primero
settings.application.class.name.error.title=Error en selector de clase
settings.application.hint=Argumentos pasados al método main de la clase principal (si existen).
settings.beforeShellScript=Antes del script de submit
settings.beforeShellScript.hint=Script a ejecutar antes de Spark Submit. Por ejemplo, "source activate py36"
settings.cluster.manager=Gestor de cluster\:
settings.cluster.manager.proxy.user=Usuario proxy\:
settings.cluster.manager.proxy.user.hint=<html>--proxy-user Usuario a suplantar al enviar la aplicación.<br>Este argumento no afecta a --principal/--keytab.</html>
settings.cluster.manager.queue=Cola\:
settings.cluster.manager.queue.hint=--queue Cola YARN a la que enviar (predeterminada\: "default").
settings.cluster.manager.supervise=Habilitar supervisión
settings.cluster.manager.supervise.hint=--supervise Si se proporciona, reinicia el controlador cuando falla.
settings.debug.driver.in.debug.mode=En modo debug
settings.debug.driver.in.run.mode=En modo ejecución
settings.debug.driver.java.enable=Iniciar el driver de Spark con agente de debug
settings.debug.driver.java.not.supported=El modo de despliegue en cluster no soporta debugging
settings.debug.driver.java.port=Puerto de escucha\:
settings.debug.driver.java.port.dynamic=<dinámico>
settings.debug.driver.java.suspend=Suspender driver\:
settings.debug.driver.java.suspend.tooltip=Suspender el proceso del driver de Spark hasta que se conecte el debugger
settings.debug.driver.java.tooltip=Agregar '-agentlib\:jdwp' a las opciones Java del driver
settings.debug.title=Debug de Spark
settings.dependencies.files=Archivos\:
settings.dependencies.files.hint=--files Lista separada por comas de archivos que se colocarán en el directorio de trabajo de cada executor. Se puede acceder a las rutas de estos archivos en los executors mediante SparkFiles.get(fileName).
settings.dependencies.jars=Jars\:
settings.dependencies.jars.hint=--jars Lista separada por comas de jars que se incluirán en el classpath del driver y los executors.
settings.dependencies.python=Archivos Py\:
settings.dependencies.python.hint=--py-files Lista separada por comas de archivos .zip, .egg o .py para agregar al PYTHONPATH de la aplicación Python.
settings.dependencies.title=Dependencias
settings.deploy.mode=Modo de despliegue\:
settings.deploy.mode.hint=<html>--deploy-mode Si el driver se inicia localmente ("client") o en una<br>de las máquinas worker dentro del cluster ("cluster").</html>
settings.deploymode.client=cliente
settings.deploymode.cluster=cluster
settings.driver.class.path=Classpath del driver\:
settings.driver.class.path.hint=<html>--driver-class-path Entradas adicionales del classpath para pasar al driver<br>Note que los jars agregados con --jars se incluyen automáticamente en el classpath.</html>
settings.driver.cores=Cores del driver\:
settings.driver.cores.hint=--driver-cores Número de cores usados por el driver, solo en modo cluster.
settings.driver.java.options=Opciones Java del driver\:
settings.driver.java.options.hint=--driver-java-options Opciones Java adicionales para pasar al driver.
settings.driver.library.path=Library path del driver\:
settings.driver.library.path.hint=--driver-library-path Entradas adicionales del library path para pasar al driver.
settings.driver.memory=Memoria del driver\:
settings.driver.memory.hint=--driver-memory Memoria para el driver (ej. 1000M, 2G).
settings.driver.title=Driver
settings.envParams=Variables de entorno
settings.envParams.hint=Variables de entorno adicionales
settings.executor.archives=Archivos\:
settings.executor.archives.hint=<html>--archives Lista de archivos a extraer en el directorio de trabajo de cada executor.</html>
settings.executor.cores=Cores por executor\:
settings.executor.cores.hint=<html>--executor-cores Número de cores por executor<br>(por defecto\: 1 en modo YARN, todos los cores disponibles en workers en modo standalone).</html>
settings.executor.cores.total=Total de cores de executors\:
settings.executor.cores.total.hint=--total-executor-cores Número total de cores para todos los executors.
settings.executor.memory=Memoria del executor\:
settings.executor.memory.default=1G
settings.executor.memory.hint=--executor-memory Memoria por executor (ej\: 1000M, 2G)
settings.executor.number=Número de executors\:
settings.executor.number.hint=<html>--num-executors Número de executors a lanzar<br>Si la asignación dinámica está habilitada, este será el número mínimo inicial de executors.</html>
settings.executor.title=Executor
settings.integration.spark.monitoring=Conexiones\:
settings.integration.spark.monitoring.add=Agregar
settings.integration.title=Integración de monitoreo Spark
settings.isInteractive=Interactivo
settings.isInteractive.hint=Ejecutar comandos en modo shell interactivo
settings.kerberos.keytab=Keytab\:
settings.kerberos.keytab.hint=<html>--keytab Ruta completa al archivo keytab que contiene el principal especificado arriba<br>Este keytab será copiado al nodo que ejecuta el Application Master a través de Secure Distributed Cache,<br>usado para renovar periódicamente los tickets de login y delegation tokens.</html>
settings.kerberos.principal=Principal\:
settings.kerberos.principal.hint=--principal Principal usado para login al KDC, también al ejecutar en HDFS seguro.
settings.kerberos.title=Kerberos
settings.master=Master\:
settings.master.hint=--master spark\://host\:port, mesos\://host\:port, yarn, k8s\://https\://host\:port, o local (por defecto\: local[*]).
settings.maven.exclude.packages=Excluir paquetes\:
settings.maven.exclude.packages.hint=<html>--exclude-packages Lista de groupId\:artifactId a excluir al resolver<br>dependencias proporcionadas en --packages para evitar conflictos.</html>
settings.maven.packages=Paquetes\:
settings.maven.packages.hint=<html>--packages Lista de coordenadas Maven de jars a incluir en el classpath del driver y executors<br>Se buscará en el repositorio Maven local, luego Maven Central y cualquier repositorio remoto adicional dado por --repositories<br>El formato de coordenadas debe ser groupId\:artifactId\:version.</html>
settings.maven.repositories=Repositorios\:
settings.maven.repositories.hint=--repositories Lista de repositorios remotos adicionales para buscar las coordenadas Maven dadas por --packages.
settings.maven.title=Maven
settings.run.target.config=Target remoto\:
settings.run.target.tooltip=Seleccionar un cluster Spark para ejecutar la aplicación
settings.shell.title=Opciones de Shell
settings.shellExecutor=Ruta del Shell
settings.shellExecutor.hint=Ruta al shell. Se usa si el modo interactivo está habilitado o antes de configurar el script de envío
settings.spark.config=Configuración\:
settings.spark.config.hint=--conf Propiedades de configuración Spark.
settings.spark.home=Directorio home de Spark\:
settings.spark.properties.file=Archivo de propiedades\:
settings.spark.properties.file.hint=<html>--properties-file Ruta al archivo para cargar propiedades adicionales.<br>Si no se especifica, se buscará conf/spark-defaults.conf.</html>
settings.spark.python.sdk=Ejecutar con entorno Python\:
settings.spark.title=Configuración Spark
settings.ssh.config=Configuración SSH\:
settings.ssh.error.msg=Por favor seleccione primero una configuración SSH
settings.ssh.error.title=Error del selector de archivos
settings.ssh.target.dir=Directorio de carga destino\:
settings.ssh.target.dir.hint=<html>Ingrese la ruta del directorio en el host remoto donde se cargarán todos los archivos locales.<br>Si los archivos seleccionados ya existen en este directorio, serán sobrescritos.</html>
settings.ssh.title=Opciones SFTP
settings.url.artifact.name=Artefacto IDEA
settings.url.artifact.tooltip=Artefacto IDEA
settings.url.custom.name=Personalizado
settings.url.file.name=Archivo
settings.url.gcs.name=GC Storage
settings.url.gcs.tooltip=GC Storage
settings.url.gradle.artifact.name=Artefacto Gradle
settings.url.gradle.artifact.tooltip=Artefacto Gradle
settings.url.hdfs.name=HDFS
settings.url.maven.artifact.name=Artefacto Maven
settings.url.maven.artifact.tooltip=Artefacto Maven
settings.url.s3.name=S3
settings.url.server.mock.desc=Ruta del archivo\:
settings.url.server.name=Archivo del servidor
settings.url.server.tooltip=Archivo del servidor
settings.url.upload.name=Subir archivo
settings.url.upload.tooltip=Subir archivo local
settings.url.web.name=Remoto
settings.workingDirectory=Directorio de trabajo
setup.ssh.config=Configurar SSH
spark.converter.build.run.configuration.description=La configuración de ejecución 'PySpark' ha cambiado. Es necesario convertir las configuraciones existentes.
spark.submit.gutter.icon.tooltip=Ejecutar en cluster Spark
sparkhome.tooltip=Directorio Spark
upload.file.title=Subir archivo al remoto
upload.files.error=Excepción al subir archivos. {0}
upload.files.success={0} archivos subidos exitosamente
upload.files.through.sftp.to.spark.host=Subir archivos vía SFTP
upload.target.dir.is.not.found=Directorio destino "{0}" no encontrado
work.directory.tooltip=Apunta a la ubicación de invocación del script