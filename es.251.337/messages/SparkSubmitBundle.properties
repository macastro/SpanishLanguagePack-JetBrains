action.BigDataTools.CreateClusterConnectionsAction.text=Crear nueva conexión
action.BigDataTools.Deploy.Configure.text=Crear configuración de envío de Spark...
add.new.arbitrary.cluster.submit.connection.label=Agregar clúster Spark personalizado...
add.new.ssh.connection.label=Añadir conexión SSH...
app.by.me.value=Por mí
arbitrary.cluster.wizard.create.connection.title=Clúster Spark personalizado
arbitrary.cluster.wizard.default.radio.button=Usar configuración predeterminada
arbitrary.cluster.wizard.select.sftp.step.desc=Establece una conexión SFTP con el nodo del controlador
arbitrary.cluster.wizard.select.sftp.step.title=SFTP
arbitrary.cluster.wizard.select.spark.step.desc=Establecer una conexión al Servidor Spark History
arbitrary.cluster.wizard.select.spark.step.title=Spark
arbitrary.cluster.wizard.select.ssh.step.desc=Especifique la configuración SSH para usar Spark-Submit
arbitrary.cluster.wizard.select.ssh.step.title=SSH
arbitrary.cluster.wizard.sftp.custom.radio.button=Establecer conexión personalizada
arbitrary.cluster.wizard.sftp.none.radio.button=No necesito una conexión SFTP al nodo del controlador
arbitrary.cluster.wizard.spark.custom.radio.button=Establecer conexión personalizada
arbitrary.cluster.wizard.spark.none.radio.button=No se necesita la conexión al servidor de historial de Spark
arbitrary.cluster.wizard.spark.step.default.uri.comment.text=(localhost del host del túnel)
artifact.tooltip=Ruta del ejecutable a ejecutar en el cluster
check.connection.availability=Comprobar disponibilidad de la conexión
check.ssh.connection.ssh.is.not.defined=La configuracion ssh no esta definida
cluster.manager.kubernetes=Kubernetes
cluster.manager.local=local
cluster.manager.mesos=Apache Mesos
cluster.manager.nomad=Nomad
cluster.manager.standalone=independiente
cluster.manager.tooltip=Gestor de clúster configurado en el servidor.
cluster.manager.yarn=Hadoop YARN
cluster.status.loading=Cargando...
cluster.status.no.target=Sin objetivo
cluster.status.not.selected=<No seleccionado>
configuration.description=Configuración Spark Submit
configuration.name=Spark Submit
configuration.name.cluster=clúster
configuration.name.local=Local (en desuso)
configuration.name.python=PySpark
configuration.name.ssh=SSH (en desuso)
configuration.options.add=Otros personalizados
configuration.options.add.title=Agregar opción de confirmación
dialog.archives.title=Seleccionar archivo de almacenamiento
dialog.artifactPath.title=Seleccionar aplicación
dialog.driverClassPath.title=Seleccionar ruta de la clase del driver
dialog.driverLibraryPath.title=Seleccionar ruta de acceso a la biblioteca del controlador
dialog.files.title=Selector de archivos
dialog.input.spark.command.description=Pega aquí un comando spark-submit para poblar el formulario Spark Submit
dialog.input.spark.command.label=Comando Spark
dialog.input.spark.command.title=Entrada Spark
dialog.jars.title=Seleccionar archivo Jar
dialog.keytabFile.title=Seleccionar archivo Keytab
dialog.message.adding.other.configurations.not.allowed.here=No se permite agregar otras configuraciones aquí
dialog.message.failed.to.create.ssh.process=No se pudo crear el proceso SSH
dialog.message.not.found={0} no encontrado
dialog.message.spark.home.should.be.set.to.correct.folder=$SPARKHOME debe establecerse en la carpeta correcta
dialog.message.specify.application=Especificar aplicación
dialog.message.ssh.configuration.changed=Ha seleccionado otra configuración SSH para el clúster {0}.
dialog.propertiesFile.title=Selector de archivo de propiedades
dialog.pyfiles.title=Seleccionar archivos Python
dialog.select.artifact.button.open.artifact.settings=Configuración de artefacto
dialog.select.artifact.empty=El proyecto actual no tiene artefactos
dialog.select.artifact.link.open.artifact.settings=Configurar artefacto
dialog.select.artifact.title=Seleccionar artefacto dependiente
dialog.select.class.empty=No se encontró ninguna clase en la aplicación seleccionada
dialog.select.class.title=Nombre de clase
dialog.sparkHomePath.title=Seleccionar directorio base de Spark
dialog.targetDirectory.title=Seleccionar directorio de destino
dialog.title.select.gradle.artifact.with.task=Seleccionar artefacto y tarea de Gradle
dialog.title.ssh.configuration.changed=Configuración SSH modificada
dialog.workDir.title=Selecciona el directorio de trabajo
edit.ssh.configuration=Editar configuración SSH
error.ssh=Configuración SSH especificar
error.ssh.config=La configuración SSH debe especificarse
error.target.config=Seleccionar destino remoto
error.target.config.unresolved=Seleccionar un objetivo remoto válido
exportable.SparkSubmitSettings.presentable.name=Big Data Tools Spark Submit Ajustes
fun.search.process.text=Proceso {0}
group.ServiceView.AddSparkService.text=Clúster Spark
inlay.attach.debugger.ssh=Adjuntar depurador mediante túnel SSH
label.implicit.cluster.depend.sftp=SFTP:
label.implicit.cluster.depend.spark.connection=Historial de Spark:
label.select.ssh.configuration.for.cluster=Seleccionar configuración SSH para grupo {0}
load.command.string=Cargar el comando spark-submit
notification.group.sftpsparkfileupload=Carga de archivos SFTP de Spark
open.cluster.info.description=Haz clic para abrir el documento
progress.text.upload.to.host=Subiendo {0} al host...
pyspark=PySpark
pyspark.documentation.dataframe.schema=Columnas: {0}
receive.artifact.task=Recibiendo artefacto...
remote.target.arbitrary.cluster.remark=Clúster arbitrario
remote.target.ssh.remark=SSH
replace.with.allowed.value=Reemplazar con valor permitido
row.final.command=Comando de envío del resultado
row.final.command.copy=Copiar comando spark-submit
row.final.command.hint=Puede copiar o cargar un comando spark-submit, que rellenará los campos correspondientes
services.error.create.spark.connection.canceled.fix.label=Intentar cargar de nuevo
services.error.create.spark.connection.canceled.message=Cancelado por el usuario
services.error.ssh.config.is.not.found=La configuración de SSH no se encuentra
services.error.top.cannot.create.spark.connection=No se puede crear conexión Spark
services.spark.connection.is.not.created=No se pudo crear la conexión a Spark
settings.additional.title=Opciones avanzadas de enviar
settings.additional.verbose=Imprime información de depuración adicional
settings.application=Aplicación:
settings.application.arguments=Argumentos del programa:
settings.application.class.hint=--class La clase principal de su aplicación (para aplicaciones Java / Scala).
settings.application.class.name=Clase:
settings.application.class.name.error.msg=Primero especifique la aplicación
settings.application.class.name.error.title=Error en el selector de clase
settings.application.hint=Argumentos al método main de la clase principal (si existen).
settings.beforeShellScript=Script antes de enviar
settings.beforeShellScript.hint=Script que se ejecuta antes de Spark Submit. Por ejemplo, "source activate py36"
settings.cluster.manager=Administrador de Clúster:
settings.cluster.manager.proxy.user=Usuario proxy:
settings.cluster.manager.proxy.user.hint=<html>--proxy-user Usuario a imitar al enviar la aplicación.<br>Este argumento no tiene efecto al usarse con --principal/--keytab.</html>
settings.cluster.manager.queue=Cola:
settings.cluster.manager.queue.hint=--queue Cola de YARN a la que enviar (valor predeterminado: "default")
settings.cluster.manager.supervise=supervisión habilitada
settings.cluster.manager.supervise.hint=--supervise reiniciará al driver si falla cuando sea especificado.
settings.debug.driver.in.debug.mode=En modo de depuración
settings.debug.driver.in.run.mode=En modo "run"
settings.debug.driver.java.enable=Iniciar controlador Spark con agente de depuración
settings.debug.driver.java.not.supported=El modo de despliegue del cluster no soporta la depuración
settings.debug.driver.java.port=Puerto de escucha:
settings.debug.driver.java.port.dynamic=<dinámico>
settings.debug.driver.java.suspend=Suspensión del controlador:
settings.debug.driver.java.suspend.tooltip=Suspend process of Spark driver until debugger is attached
settings.debug.driver.java.tooltip=Agrega '-agentlib:jdwp' a las opciones de Java del driver
settings.debug.title=Depuración de Spark
settings.dependencies.files=Archivos:
settings.dependencies.files.hint=--files la lista de archivos separados por comas que se colocarán en el directorio de trabajo de cada ejecutor. Los archivos en estas rutas ejecutoras están disponibles a través de SparkFiles.get(fileName)
settings.dependencies.jars=JAR:
settings.dependencies.jars.hint=--jars Lista separada por comas de jars a incluir en la ruta de clase del controlador y el ejecutable.
settings.dependencies.python=Archivos Py:
settings.dependencies.python.hint=--py-files Lista separada por comas de archivos .zip, .egg o .py que se agregarán a PYTHONPATH de la aplicación Python
settings.dependencies.title=Dependencias
settings.deploy.mode=Modo de despliegue:
settings.deploy.mode.hint=<html>--deploy-mode es si el controlador se inicia localmente ("cliente") o en una de las<br>máquinas de procesamiento de trabajo dentro del clúster ("clúster").</html>
settings.deploymode.client=Cliente
settings.deploymode.cluster=cluster
settings.driver.class.path=Ruta de la clase del controlador:
settings.driver.class.path.hint=<html>--driver-class-path elementos de ruta de clase adicionales para pasar al controlador<br>Los jar añadidos con --jars se incluyen automáticamente en la ruta de clase.</html>
settings.driver.cores=Núcleos del driver:
settings.driver.cores.hint=--driver-cores Número de núcleos usados por el controlador, solo en modo clúster.
settings.driver.java.options=Opciones de Java del controlador:
settings.driver.java.options.hint=--driver-java-options Opciones Java adicionales que se pasan al controlador.
settings.driver.library.path=Ruta de la biblioteca del controlador:
settings.driver.library.path.hint=--driver-library-path Entrada de ruta de biblioteca adicional para pasar al controlador.
settings.driver.memory=Memoria del Driver:
settings.driver.memory.hint=-memoria-de-controlador --driver-memory (por ejemplo, 1000M, 2G)
settings.driver.title=Controlador
settings.envParams=Variables de entorno
settings.envParams.hint=Variables de entorno adicionales
settings.executor.archives=Archivos:
settings.executor.archives.hint=<html>--archives Listado de archivos comprimidos que se deben extraer en los directorios de trabajo de cada ejecutor.</html>
settings.executor.cores=númnúcleosexecutor:
settings.executor.cores.hint=<html>--executor-cores Número de núcleos por Executor<br>(Valor predeterminado: 1 en modo YARN, todos los núcleos disponibles en trabajo en modo independiente).</html>
settings.executor.cores.total=Total de núcleos del ejecutor:
settings.executor.cores.total.hint=--total-executor-cores Número total de cores de todos los ejecutores.
settings.executor.memory=Memoria del ejecutor:
settings.executor.memory.default=1G
settings.executor.memory.hint=--executor-memory Memoria por ejecutor (por ejemplo: 1000M, 2G)
settings.executor.number=Número de ejecutores:
settings.executor.number.hint=<html>--num-executors Número de ejecutores a iniciar<br>Si la asignación dinámica está habilitada, el número inicial de ejecutores será al menos este valor.</html>
settings.executor.title=Ejecutor
settings.integration.spark.monitoring=Conectar:
settings.integration.spark.monitoring.add=Añadir nuevo
settings.integration.title=Integración de monitorización de Spark
settings.isInteractive=interactivo
settings.isInteractive.hint=Ejecutar el comando en modo interactivo de shell
settings.kerberos.keytab=keytab:
settings.kerberos.keytab.hint=<html>--keytab Ruta completa al archivo que contiene el keytab del principal especificado anteriormente<br>Este keytab se copiará a los nodos donde se ejecuta el Application Master vía Secure Distributed Cache,<br>para actualizar periódicamente los tickets de acceso y los tokens delegados.</html>
settings.kerberos.principal=Principal:
settings.kerberos.principal.hint=--principal Entidad usada para iniciar sesión en el KDC y en el sujeto que se ejecuta en un HDFS seguro.
settings.kerberos.title=Kerberos
settings.master=Principal:
settings.master.hint=--master spark://host:port, mesos://host:port, yarn, k8s://https://host:port o local (por defecto: local[*])
settings.maven.exclude.packages=Excluir paquetes:
settings.maven.exclude.packages.hint=<html>--exclude-packages Lista de groupId:artifactId para excluir al resolver las dependencias proporcionadas en --packages,<br>para evitar conflictos de dependencias.</html>
settings.maven.packages=Paquetes:
settings.maven.packages.hint=<html>--packages lista de coordenadas Maven de jar que se incluirán en la ruta de clase del controlador y el ejecutor<br>Se buscarán en el repositorio local de Maven, seguidamente en el repositorio central de Maven y en cualquier otro repositorio remoto facilitado en --repositories<br>El formato de coordenadas debe ser groupId:artifactId:version</html>
settings.maven.repositories=Repositorios:
settings.maven.repositories.hint=--repositories Lista de repositorios remotos adicionales para buscar las coordenadas Maven indicadas en --packages.
settings.maven.title=Maven
settings.run.target.config=Objetivo remoto:
settings.run.target.tooltip=Selecciona un clúster Spark para ejecutar la aplicación
settings.shell.title=Opciones de Shell
settings.shellExecutor=Ruta del shell
settings.shellExecutor.hint=Ruta al shell. Se utiliza si se habilita el modo interactivo o si se utiliza el script de envío para ajuste
settings.spark.config=Configuración:
settings.spark.config.hint=--conf Spark Config Propiedades.
settings.spark.home=Ruta local de Spark:
settings.spark.properties.file=Archivo de propiedades:
settings.spark.properties.file.hint=<html>--fichero-propiedades-cargar ruta de fichero para cargar propiedades adicionales.<br>Si no se especifica, se buscará conf/spark-defaults.conf.</html>
settings.spark.python.sdk=Ejecutar en entorno Python:
settings.spark.title=Spark configuración
settings.ssh.config=Configuración SSH:
settings.ssh.error.msg=Por favor, selecciona primero la configuración SSH
settings.ssh.error.title=Error de selector de archivos
settings.ssh.target.dir=Directorio de subida objetivo:
settings.ssh.target.dir.hint=<html>Introduce la ruta del directorio en el host remoto en el cual se subirán todos los archivos locales. <br>Si los archivos seleccionados ya existen en este directorio, serán sobreescritos. </html>
settings.ssh.title=Opciones de SFTP
settings.url.artifact.name=IDEA Artefacto
settings.url.artifact.tooltip=Artifact de IDEA
settings.url.custom.name=Personalizado
settings.url.file.name=Archivo
settings.url.gcs.name=Almacenamiento GC
settings.url.gcs.tooltip=Almacenamiento GC
settings.url.gradle.artifact.name=Nombre de artefacto de Gradle
settings.url.gradle.artifact.tooltip=Artefacto Gradle
settings.url.hdfs.name=HDFS
settings.url.maven.artifact.name=Nombre del artefacto Maven
settings.url.maven.artifact.tooltip=Artefacto de Maven
settings.url.s3.name=S3
settings.url.server.mock.desc=Ruta del archivo:
settings.url.server.name=Archivo del servidor
settings.url.server.tooltip=Archivo del servidor
settings.url.upload.name=Subir archivo
settings.url.upload.tooltip=Sube un fichero local
settings.url.web.name=Remoto
settings.workingDirectory=Directorio de trabajo
setup.ssh.config=Configuración SSH
spark.converter.build.run.configuration.description='PySpark' La configuración de ejecución ha cambiado. Se requiere convertir la configuración existente.
spark.submit.gutter.icon.tooltip=Ejecutar en clúster Spark
sparkhome.tooltip=Directorio Spark
upload.file.title=Subir fichero a remoto
upload.files.error=Excepción al cargar los archivos. {0}
upload.files.success={0} archivos subidos exitosamente
upload.files.through.sftp.to.spark.host=Subir archivo por SFTP
upload.target.dir.is.not.found=No se encontró el directorio objetivo "{0}"
work.directory.tooltip=Apunta al directorio de donde se llama el script