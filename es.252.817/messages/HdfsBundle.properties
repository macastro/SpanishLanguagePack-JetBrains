alibaba.bucket.create.dialog.hierarchical.field=Espacio de nombres jerárquico
alibaba.bucket.create.dialog.versioning=Habilitar control de versiones
alibaba.custom.endpoint.text=Personalizado
alibaba.region.oss-ap-northeast-1=Japón (Tokio)
alibaba.region.oss-ap-northeast-2=Corea (Seúl)
alibaba.region.oss-ap-south-1=India (Mumbai)
alibaba.region.oss-ap-southeast-1=Australia (Sídney) 1
alibaba.region.oss-ap-southeast-2=Australia (Sídney) 2
alibaba.region.oss-ap-southeast-3=Malasia (Kuala Lumpur)
alibaba.region.oss-ap-southeast-5=Indonesia (Yakarta)
alibaba.region.oss-ap-southeast-6=Filipinas (Manila)
alibaba.region.oss-ap-southeast-7=Tailandia (Bangkok)
alibaba.region.oss-cn-beijing=China (Pekín)
alibaba.region.oss-cn-chengdu=China (Chengdu)
alibaba.region.oss-cn-guangzhou=China (Guangzhou)
alibaba.region.oss-cn-hangzhou=China (Hangzhou)
alibaba.region.oss-cn-heyuan=China (Heyuan)
alibaba.region.oss-cn-hongkong=China (Hong Kong)
alibaba.region.oss-cn-huhehaote=China (Hohhot)
alibaba.region.oss-cn-nanjing=China (Nanjing - Región local)
alibaba.region.oss-cn-qingdao=China (Qingdao)
alibaba.region.oss-cn-shanghai=China (Shanghai)
alibaba.region.oss-cn-shenzhen=China (Shenzhen)
alibaba.region.oss-cn-wulanchabu=China (Ulanqab)
alibaba.region.oss-cn-zhangjiakou=China (Zhangjiakou)
alibaba.region.oss-eu-central-1=Alemania (Fráncfort)
alibaba.region.oss-eu-west-1=Reino Unido (Londres)
alibaba.region.oss-me-east-1=EAU (Dubái)
alibaba.region.oss-us-east-1=EE.UU. (Virginia)
alibaba.region.oss-us-west-1=EE.UU. (Silicon Valley)
alibaba.settings.credentials.file=Archivo de credenciales de Alibaba
alibaba.task.delete.bucket.text=Eliminando bucket {0}
alibaba.task.delete.directory.text=Eliminando directorio {0}
alibaba.task.delete.directory.text2=Eliminados {0} objetos
alibaba.task.delete.file.text=Eliminando archivo {0}
azure.column.name.access.tier=Nivel de acceso
azure.rename.text2.indicator.deleting={0}\: Eliminando {1}
bucket.name.is.empty.for.path=El nombre del bucket está vacío para la ruta "{0}"
cannot.find.linode.region=No se puede encontrar la región para {0}
cannot.open.read.stream.null.blob=No se puede abrir el stream de lectura para el blob nulo {0}
client.is.not.inited=El cliente no está inicializado
connection.error.fs.and.user.not.found=No se encuentra el sistema de archivos para URI {0} y usuario {1}
connection.error.hadoop.home.is.not.defined.full=HADOOP_HOME no está definido. En Windows, debe definir la variable de entorno HADOOP_HOME o la propiedad Java hadoop.home.dir. Consulte <a href\="https\://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems">Hadoop Wiki</a> para más detalles.
connection.error.hadoop.no.native.drivers.full=No se encuentran los drivers nativos en HADOOP_HOME. Consulte <a href\="https\://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems">Hadoop Wiki</a> para más detalles.
connection.error.root.path.must.be.non.empty=La ruta raíz no puede estar vacía
controller.cluster.instances.error=Error al actualizar instancias del cluster
controller.cluster.steps.error=Error al actualizar pasos del cluster
copy.failed=Error al copiar de {0} a {1}
custom.bucket.text.empty=bucket/folder,bucket2/folder/subfolder2,…
custom.bucket.text.hint=Especifique lista de raíces origen usando separador "," (bucket1/folder1/folder2,bucket2/folder)
do.region.ams3=Ámsterdam, Países Bajos
do.region.blr1=Berlín, Alemania
do.region.fra1=Fráncfort, Alemania
do.region.nyc3=Nueva York, EE.UU.
do.region.sfo=San Francisco, EE.UU.
do.region.sgp1=Singapur
do.region.syd1=Sídney, Australia
emr.cluster.filter=Filtrar por estado
emr.cluster.filter.limit=Límite
emr.cluster.info.details=Mostrar como JSON
emr.cluster.terminate.cluster.message=¿Desea terminar el cluster {0}?
emr.cluster.terminate.cluster.title=Terminando cluster
emr.connection.creation=Creación de conexión EMR
emr.connection.warning.no.clusters=Conectado, no se encontraron clusters
emr.connection.warning.no.clusters.desc=Conexión establecida pero no se encontraron clusters para la región seleccionada. Por favor verifique que la región sea correcta.
emr.connection.warning.no.clusters.desc.window=No se encontraron clusters en la región "{0}". Por favor verifique la región.
emr.dialog.title.select.key.info.cancel=Cancelar
emr.dialog.title.select.key.info.msg=La conexión requiere crear un túnel SSH. Para configurarlo, seleccione el archivo de clave SSH para el cluster.
emr.dialog.title.select.key.info.ok=Seleccionar clave SSH
emr.dialog.title.select.key.info.title=Se requiere clave SSH
emr.dialog.title.select.key.ssh.file=Seleccionar archivo de clave SSH
emr.error=Excepción AWS EMR
emr.error.remove.cluster=Error al eliminar cluster
emr.error.start.cluster=Error al iniciar cluster
emr.error.stop.cluster=Error al detener cluster
emr.filter.text=Filtro\:
emr.is.not.inited=Cliente EMR no inicializado
emr.key.storage.dialog.title=Almacén de claves SSH EMR
emr.keys.settings.column.key.name=Nombre de clave
emr.keys.settings.column.key.path=Ruta
emr.keys.settings.label=Claves SSH\:
emr.keys.settings.table.empty=No se proporcionaron claves SSH
emr.label.choose.key.file.for.aws.pair=Elegir archivo de clave para el par AWS {0}
emr.remove.linked.connections.action=Eliminar conexiones
emr.remove.linked.connections.desc=¿Desea eliminar las conexiones creadas para EMR?
emr.remove.linked.connections.title=Conexiones EMR
emr.spark.submit=EMR Spark-submit
emr.spark.submit.editor.args=Argumentos\:
emr.spark.submit.editor.jar.loc=Ubicación del JAR\:
emr.spark.submit.editor.name=Nombre\:
emr.step.details=Mostrar detalles del paso
emr.step.mapper.choose=Elegir Mapper
emr.step.reducer.choose=Elegir Reducer
emr.step.s3.input.choose=Elegir entrada S3
emr.step.s3.output.choose=Elegir salida S3
emr.step.script.choose=Elegir ubicación del script S3
emr.toolwindow.title=AWS EMR
error.krb5.conf=No se pudo autorizar mediante Kerberos. Intente agregar "allow_weak_crypto \= true" a krb5.conf
error.object.summary.is.not.found=No se encuentra el resumen del objeto para {0}
file.info.access.blob.type=Tipo de Blob
file.info.access.content.type=Tipo de contenido
file.info.access.tier=Nivel de acceso
file.info.access.tier.modified=Última modificación del nivel de acceso
gcs.buckets.source=Origen de buckets\:
gcs.connection.browse.title=Seleccionar JSON de credenciales
gcs.connection.error.bucket.validation1=El nombre del bucket debe tener entre 3 y 63 caracteres. Los nombres con puntos pueden tener hasta 222 caracteres, pero cada componente separado por puntos no debe exceder los 63 caracteres.
gcs.connection.error.bucket.validation2=El nombre solo puede contener letras minúsculas, números, guiones (-), guiones bajos (_) y puntos (.).
gcs.connection.error.bucket.validation3=El nombre del bucket debe comenzar y terminar con un número o letra.
gcs.connection.error.bucket.validation4=El nombre del bucket no puede ser una dirección IP en formato decimal con puntos (por ejemplo, 192.168.5.4).
gcs.connection.error.bucket.validation5=El nombre del bucket no puede comenzar con el prefijo "goog".
gcs.connection.error.bucket.validation6=El nombre del bucket no puede contener "google" o errores de ortografía similares como "g00gle".
gcs.connection.error.cred.file.not.selected=Debe seleccionar un archivo de credenciales para la cuenta
gcs.connection.error.file.not.exists=El archivo no existe
gcs.custom.url=Host personalizado\:
gcs.json.location.emptyText=Ubicación del JSON de Cloud Storage
gcs.multibucket.update.text=¡Google Cloud Storage soporta múltiples buckets\! Puede configurarlos en los ajustes de conexión.
gcs.multibucket.update.title=Nueva función GCS para BigDataTools
gcs.progress.details.deleting=Eliminando {0}
gcs.project.id=ID del proyecto\:
gcs.project.id.emptyText=ID de proyecto de anulación opcional
gcs.project.id.hint=Mostrar buckets para un ID de proyecto específico
gcs.public.hint=Dejar vacío para buckets públicos
gcs.sdk.install=No se encuentra Google Cloud SDK. <a>Instalar</a>
gcs.sdk.update=Google Cloud SDK está desactualizado. <a>Actualizar</a>
group.name.alibaba=Alibaba OSS
group.name.azure=Azure
group.name.dospaces=DigitalOcean Spaces
group.name.emr=AWS EMR
group.name.gcs=Google Cloud Storage
group.name.hdfs.java=HDFS
group.name.linode=Linode
group.name.minio=MinIO
group.name.s3=AWS S3
group.name.yandex=Almacenamiento de objetos Yandex
group.names.hdfs.data=Problemas HDFS
hdfs.column.name.access.time=Tiempo de acceso
hdfs.column.name.block.size=Tamaño de bloque
hdfs.column.name.group=Grupo
hdfs.column.name.is.encrypted=Encriptado
hdfs.column.name.is.isErasureCoded=Codificación de borrado
hdfs.column.name.is.isSnapshotEnabled=Snapshot
hdfs.column.name.owner=Propietario
hdfs.column.name.permission=Permisos
hdfs.column.name.replications=Réplicas
hdfs.config.path.does.not.exist=El directorio especificado no existe
hdfs.config.path.no.xmls.found=El directorio especificado no contiene archivos XML
hdfs.config.path.not.empty=La ruta de configuración no debe estar vacía
hdfs.config.path.should.be.directory=La ruta de configuración debe apuntar a un directorio
hdfs.config.path.title=Ruta de configuración de Java API
hdfs.field.root.path=Ruta raíz
hdfs.file.info.label.accessTime=Tiempo de acceso\:
hdfs.file.info.label.block.size=Tamaño de bloque\:
hdfs.file.info.label.group=Grupo\:
hdfs.file.info.label.isEncrypted=Encriptado\:
hdfs.file.info.label.isErasureCoded=Codificación de borrado\:
hdfs.file.info.label.isSnapshotEnabled=Snapshot habilitado\:
hdfs.file.info.label.modificationTime=Tiempo de modificación\:
hdfs.file.info.label.owner=Propietario\:
hdfs.file.info.label.permission=Permisos\:
hdfs.file.info.label.replication=Réplicas\:
hdfs.file.info.label.size=Tamaño\:
hdfs.is.not.inited=Conexión Hdfs no inicializada
hdfs.java.config.source=Fuente de configuración\:
hdfs.java.driver.home.path=Ruta principal del driver\:
hdfs.no.xmls.in.directory=No hay archivos XML en la raíz de configuración
hdfs.property.source.directory=Carpeta de configuración
hdfs.property.source.explicit=Personalizado
hdfs.root.folder.does.not.exist=La carpeta raíz {0} no existe
hdfs.ssh.tunnel.ssh.operation.not.supported=Operación a través de túnel SSH no soportada.
inspection.java.custom.hdfs.format.display.name=Resaltar formato de archivo HDFS personalizado
inspection.java.invalid.file.path.display.name=Resaltar ruta de archivo HDFS inválida
inspection.kotlin.custom.hdfs.format.display.name=Resaltar formato de archivo HDFS personalizado
inspection.kt.invalid.file.path.display.name=Resaltar ruta de archivo HDFS inválida
inspection.non.serializable.data.in.scope.display.name=Resaltar datos no serializables en tareas Spark
inspection.scala.custom.hdfs.format.display.name=Resaltar formato de archivo HDFS personalizado
inspection.scala.invalid.hdfs.file.path.display.name=Resaltar ruta de archivo HDFS inválida
invalid.format.inspection.description=Resalta formatos hdfs personalizados. Por defecto, los formatos esperados son parquet, orc, sequence, json, csv o text.
invalid.format.inspection.template=Formato de archivo personalizado inesperado
java.wrong.path.inspection.description=Resalta rutas hdfs inválidas en código Java
kerberos.type.credentials=Contraseña
kerberos.type.disabled=Deshabilitado
kerberos.type.keytab=Keytab
kerberos.type.subject=Configuración JAAS (experto)
kotlin.wrong.path.inspection.description=Resalta rutas hdfs inválidas en código Kotlin
linode.region.ap-south=Singapur
linode.region.br-gru-1=São Paulo, Brasil
linode.region.eu-central=Frankfurt, Alemania
linode.region.fr-par-1=París, Francia
linode.region.id-cgk-1=Yakarta, Indonesia
linode.region.in-maa-1=Chennai, India
linode.region.it-mil-1=Milán, Italia
linode.region.jp-osa-1=Osaka, Japón
linode.region.nl-ams-1=Ámsterdam, Países Bajos
linode.region.se-sto-1=Estocolmo, Suecia
linode.region.us-east=Newark, Nueva Jersey, EE.UU.
linode.region.us-iad-1=Washington DC, EE.UU.
linode.region.us-lax-1=Los Ángeles, California, EE.UU.
linode.region.us-mia-1=Miami, Florida, EE.UU.
linode.region.us-ord-1=Chicago, Illinois, EE.UU.
linode.region.us-sea-1=Seattle, Washington, EE.UU.
linode.region.us-southeast=Atlanta, Georgia, EE.UU.
metainfo.headers.empty=Sin headers personalizados
metainfo.headers.key=Clave
metainfo.headers.value=Valor
metainfo.section.custom.headers=Headers
minio.region.text.empty=Usar valor predeterminado
move.failed=Error al mover de {0} a {1}.
notification.group.orc.files=Archivos ORC
oss.file.info.label.hns.status=Namespace jerárquico\:
oss.file.info.label.hns.status.disabled=Deshabilitado
oss.file.info.label.type=Content-Type\:
rfs.create.bucket.message=Crear bucket
s3.bucket.text.empty=Todos los buckets visibles
s3.bucket.text.hint=Si este campo está vacío, todos los buckets serán visibles<br>Ingrese el nombre del bucket y seleccione el tipo de filtro "Coincide" para trabajar con un solo bucket<br>Use "," para separar buckets (bucket1, bucket2)
s3.column.name.etag=ETag
s3.column.name.metadata=Metadata
s3.column.name.storage.class=Clase de almacenamiento
s3.connection.error.ssh.without.endpoint=Para usar túnel SSH, especifique un endpoint para el driver
s3.empty.directories.not.allowed=No se permite crear directorios vacíos
s3.multibucket.open.settings=Abrir configuración
s3.multibucket.update.text=El almacenamiento compatible con S3 admite múltiples buckets. Puede configurarlos en la configuración de conexión.
s3.multibucket.update.title=Nueva funcionalidad S3 para BigDataTools
scala.serializable.scope.inspection.description=Resalta valores no serializables utilizados dentro del scope de tareas spark que causarán excepciones en tiempo de ejecución.
scala.serializable.scope.inspection.warning=<html>Valor no serializable {0} de tipo {1} en scope Spark {2}</html>
scala.wrong.path.inspection.description=Resalta rutas hdfs inválidas en código Scala
settings.alibaba.region=Región\:
settings.azure.auth.type=Tipo de autenticación\:
settings.azure.connection.string=Connection string\:
settings.azure.container=Contenedor\:
settings.azure.endpoint=Endpoint\:
settings.azure.password=Contraseña\:
settings.azure.sas.token=Token SAS\:
settings.azure.user.key=Clave\:
settings.azure.username=Usuario\:
settings.bucket.filter=Filtro de buckets\:
settings.bucket.filter.type=Tipo de filtro\:
settings.buckets.custom.list=Raíces personalizadas
settings.buckets.hint=<html><b>Todos los buckets en la cuenta</b> - Ejecuta una solicitud <it>list buckets</it>. Permite filtrar la lista de buckets resultante.<br><br><b>Raíces personalizadas</b> - Solicita las raíces seleccionadas directamente, permitiendo especificar no solo buckets sino rutas completas a directorios.</html>
settings.buckets.user.list=Todos los buckets en la cuenta
settings.config.from.folder=Configuración analizada\:
settings.config.path=Ruta de configuración\:
settings.custom.roots=Raíces\:
settings.generate.kerberos=Kerberos
settings.hdfs.auth.type=Autenticación\:
settings.hdfs.kerberos.type=Método de autenticación\:
settings.hdfs.kinit=Usar caché kinit
settings.hdfs.url=URI del cluster\:
settings.hdfs.username=Usuario Hadoop\:
settings.hdfs.username.hint=Nombre de usuario para iniciar sesión en el servidor. Si no se especifica, se usa la variable de entorno <i>HADOOP_USER_NAME</i>. Si esta no está definida, se usa la propiedad <i>user.name</i>. Si Kerberos está habilitado, sobrescribe estos tres valores.
settings.kerberos.auth=Autenticación\:
settings.kerberos.auth.kerberos=Kerberos
settings.kerberos.auth.none=Ninguna
settings.minio.endpoint=Endpoint\:
settings.properties=Configuración avanzada\:
settings.s3.bucket.filter.by.region=Solo buckets en la región seleccionada
settings.s3.custom.endpoint=Endpoint\:
settings.s3.custom.region=Región\:
settings.s3.custom.region.hint=Usar cuando sea necesario
settings.s3.region=Región\:
settings.s3.region.group=AWS S3
settings.s3.selection.endpoint=Almacenamiento compatible con S3
settings.undefined.path=<No inicializado>
settings.validation.kerberos.keytab.error=Se debe especificar keytab y principal
settings.validation.kerberos.password.error=Se debe especificar principal y contraseña
setup.video.tutor=Video tutorial de configuración de conexión
ssh.additional.info=El túnel SSH <b>solo funciona para operaciones de NameNode</b>\: listar archivos, obtener metainformación.<br><br>
ssh.additional.label=(Solo operaciones NameNode)
wrong.region=No se pudo encontrar la región "{0}"